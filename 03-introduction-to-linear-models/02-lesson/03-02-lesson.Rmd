---
title: "Introduction to Linear Models: 2 - Correlation"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
library(learnr)
library(tidyverse)
library(openintro)
library(emo)
library(gradethis)

knitr::opts_chunk$set(echo = FALSE, 
                      fig.align = "center", 
                      fig.height = 3, 
                      fig.width = 5,
                      message = FALSE, 
                      warning = FALSE)

anscombe <- anscombe %>%
  mutate(id = 1:nrow(.)) %>%
  gather(key = "key", value = "val", -id) %>%
  separate(key, into = c("variable", "set"), sep = 1) %>%
  spread(key = variable, value = val)

tutorial_options(
  exercise.timelimit = 60, 
  exercise.checker = gradethis::grade_learnr
  )
```

## Strength of bivariate relationships

### Correlation

In the previous lesson, we learned how to visually assess the *strength* of the relationship between two variables by examining a scatterplot. Correlation is a way to quantify the strength of that linear relationship.  

- correlation coefficient between -1 and 1
- sign of the correlations shows direction
- magnitude of the correlation shows strength

The correlation coefficient is a number between -1 and 1 that indicates the strength of a linear relationship. The *sign* of the correlation coefficient corresponds to the direction---positive or negative. The *magnitude* of the correlation corresponds to the strength. A correlation coefficient of close to 1 indicates near-perfect positive correlation, as we see in this plot.

### Near perfect correlation

A correlation coefficient of close to 1 indicates near-perfect positive correlation, as we see in this plot.

```{r plot}
n <- 100
# http://blog.revolutionanalytics.com/2016/08/simulating-form-the-bivariate-normal-distribution-in-r-1.html
# Target parameters for univariate normal distributions
mu1 <- 1; s1 <- 2
mu2 <- 1; s2 <- 8

# Parameters for bivariate normal distribution
mu <- c(mu1,mu2) # Mean 

cor_plot <- function(n, rho) {
  sigma <- matrix(c(s1^2, s1*s2*rho, s1*s2*rho, s2^2), 2) # Covariance matrix
  sim <- MASS::mvrnorm(n, mu = mu, Sigma = sigma) %>%
    as.data.frame() %>%
    rename(y = V1, x = V2)
  ggplot(sim, aes(x = x, y = y)) + 
    geom_point() + 
    geom_text(x = -20, y = 8, label = round(cor(sim$x, sim$y), 3), size = 10) + 
    ylim(c(-10, 10)) + xlim(c(-25, 25))
}

cor_plot(n, rho = 0.95)
```

### Strong

For scatterplots in which the points are more spread out, the value of the correlation coefficient is lower. Here we see a scatterplot with correlation that we might call "strong." 

```{r cor}
cor_plot(n, rho = 0.8)
```

### Moderate

Values of the correlation coefficient that are closer to 0.5 might be considered "moderate"...

```{r cor2}
cor_plot(n, rho = 0.5)
```

### Weak

...while values of the correlation coefficient closer to 0.2 might be considered "weak."

```{r cor3}
cor_plot(n, rho = 0.2)
```

### Zero

If there really is no linear relationship between the two variables, then the correlation coefficient will be close to zero. This scatterplot shows two variables that are uncorrelated. Note how knowing one of the $x$ values gives you almost no information about what the corresponding value of $y$ will be. 

```{r cor4}
cor_plot(n, rho = 0)
```

### Negative

Of tutorial, when the direction of the relationship is negative, the value of the correlation coefficient is as well.

```{r cor5}
cor_plot(n, rho = -0.5)
```

### Non-linear

It is crucial to remember that the correlation coefficient only captures the strength of the *linear* relationship between two variables. It is quite common to encounter variables that are strongly-related, but in a non-linear way. Blindly computing and reporting the correlation between such variables is not likely to yield meaningful results. 

```{r plot2}
sim <- data.frame(x = rnorm(n)) %>%
  mutate(y = 5 * x^2 + rnorm(n, sd = 1))

ggplot(sim, aes(x = x, y = y)) + 
  geom_point() + 
  geom_text(x = 0, y = 20, 
            label = round(cor(sim$x, sim$y), 3), size = 10)
```

This scatterplot shows a clear, quadratic relationship. To simply interpret the low correlation coefficient as evidence that $x$ and $y$ are unrelated would be plainly incorrect. Here, $x$ and $y$ are closely related---just not in a linear fashion.

### Non-linear correlation

Here is an example of real data from a 10-mile run. The pace of the top 10 finishers in each age group are plotted against their age. Note how the relationship between the pace and age is non-linear: 25-year-olds tend to actually be faster than 20-year-olds, but as people age beyond their thirties, they tend to get slower. This pattern is present for both men and women. 

```{r code1}
run09 %>%
  filter(div_place <= 10) %>%
  ggplot(aes(x = age, y = pace, color = gender)) + 
  geom_point()
```

```{r code2}
run09 %>%
  filter(div_place <= 10) %>%
  summarize(cor(pace, age))
```

The value of the correlation coefficient here is about 0.68, but we have some intuitive sense that pace and age are more closely-related than the figure indicates. 

### Pearson Product-Moment Correlation

There are several different ways that correlation can be defined in statistics, but by far the most common is the **Pearson Product-Moment correlation**. When we say "correlation" we are talking about this value, but you should be aware that there are other definitions that are preferred in other contexts. 

$$
  r(x,y) = \frac{Cov(x, y)}{\sqrt{SXX \cdot SYY}} 
$$


Correlation is most often denoted with the letter $r$, and it is a function of two variables, most commonly $x$ and $y$. The definition in terms of covariance - which we haven't defined - and the sums of the squares is shown here. 

### Pearson Product-Moment Correlation

This is an alternative definition using only $x$'s and $y$'s, and their means: $\bar{x}$ and $\bar{y}$. In the denominator, we see the sums of the squared deviations in both $x$ and $y$. In the numerator, we see a sum of terms involving both $x$ and $y$. In fact, each term in the sum is the area of the rectangle with corners at each data point $(x_i, y_i)$ and the means $(\bar{x}, \bar{y})$. So in some sense this is measuring the deviation from the mean in both $x$ and $y$.  


$$
  r(x,y) = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2 \cdot \sum_{i=1}^n (y_i - \bar{y})^2}
$$

In practice, we always have R do these computations for us, so memorizing the formulas is not important. For some of you, the mathematics may help reinforce your intuition about what the correlation coefficient actually measures. For others, it may simply be a confusing blur of symbols that can be safely ignored. Having a solid conceptual understanding of what correlation is is the important thing.

You'll get some more practice computing correlation in the next set of exercises. 

### Understanding correlation scale

Choose the correct interpretation of these findings.

```{r mc1}
question("In a scientific paper, three correlations are reported with the following values:
         (1) -0.395
         (2) 1.827
         (3) 0.738",
  answer("(1) is invalid.", message = "(1) is valid"),
  answer("(2) is invalid.", correct = TRUE, message ="Right! Correlation coefficients are always between -1 and 1." ),
  answer("(3) is invalid.", message = "(3) is perfectly valid!"),
  answer("Both (1) and (2) are invalid.", message = "Wrong!"),
  answer("Both (2) and (3) are invalid.", message = "Incorrect!"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  correct = "Correct!"
)
```

### Understanding correlation sign

```{r mc2}
question("In a scientific paper, three correlations are reported with the following values. Which of these values represents the strongest correlation?",
  answer("0.582"),
  answer("0.134", message = "This is the weakest correlation."),
  answer("-0.795", correct = TRUE, message = "Indeed, the closer to 1 or -1, the stronger the correlation."),
  answer("Can't tell!"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  correct = "Correct!"
)
```


### Computing correlation

The `cor(x, y)` function will compute the Pearson product-moment correlation between variables, `x` and `y`. Since this quantity is symmetric with respect to `x` and `y`, it doesn't matter in which order you put the variables. 

At the same time, the `cor()` function is very conservative when it encounters missing data (e.g. `NA`s). The `use` argument allows you to override the default behaviour of returning `NA` whenever any of the values encountered is `NA`. Setting the `use` argument to `"pairwise.complete.obs"` allows `cor()` to compute the correlation coefficient for those observations where the values of `x` and `y` are both not missing.

- Use `cor()` to compute the correlation between the birthweight of babies in the `ncbirths` dataset and their mother's age. There is no missing data in either variable.
- Compute the correlation between the birthweight and the number of weeks of gestation for all non-missing pairs.

```{r ex1-a, exercise = TRUE}
# Compute correlation
ncbirths %>%
  summarize(N = n(), r = cor(___, ___))
```

```{r ex1-a-solution}
ncbirths %>%
  summarize(N = n(), r = cor(weight, mage))
```

```{r ex1-a-check}
grade_result(
  pass_if(~identical(round(.result$r,digits = 3),0.055),"Your solution is correct!"),
  fail_if(~is.na(.result$r), "Did you perhaps use other variables instead of the weight and mother's age?"),
  fail_if(~ TRUE, "Not quite. Make sure you are computing the correlation for variables `weight` and `mage`.")
)
```

```{r ex1-b, exercise = TRUE}
# Compute correlation for all non-missing pairs
ncbirths %>%
  summarize(N = n(), r = cor(___, ___, use = ___))
```

```{r ex1-b-hint}
ncbirths %>%
  summarize(N = n(), r = cor(___, ___, use = "pairwise.complete.obs"))
```

```{r ex1-b-solution}
# Compute correlation for all non-missing pairs
ncbirths %>%
  summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))

```

```{r ex1-b-check}
grade_result(
  pass_if(~identical(round(.result$r,digits = 3),0.670),"Your solution is correct!"),
  fail_if(~is.na(.result$r), "Make sure to set the `use` argument to 'pairwise.complete.obs'. "),
  fail_if(~ TRUE, "Not quite. Look at the hint for help!")
)
```

```{r ex1, exercise = TRUE}
ncbirths %>%
  summarize(N = n(), r = cor(weight, mage))

ncbirths %>%
  summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))
```

```{r ex1-solution}
ncbirths %>%
  summarize(N = n(), r = cor(weight, mage))

ncbirths %>%
  summarize(N = n(), r = cor(weight, weeks, use = "pairwise.complete.obs"))

```

```{r ex1-check}
grade_result(
  pass_if(~identical(round(.result$r,digits=3),0.670)),
  fail_if(~TRUE,"Not quite")
)
```

## The Anscombe dataset

### Anscombe

In 1973, statistician Francis Anscombe created a synthetic data set that has been used extensively to illustrate concepts related to correlation and regression.

```{r ans}
ggplot(data = anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~set)
```

Here we see four datasets, each having a very different graphical presentation. However, these datasets have the same number of points, the same mean and standard deviation in both $x$ and $y$, the same correlation, and the same regression line. How is it possible that four datasets with such obvious visual differences could have so many identical important properties? 

The first set looks the most like "real data." Here we see a positive, linear, moderately strong relationship. As you gain experience working with real data, you will see lots of scatterplots that look similar to this one. Here, the correlation coefficient of 0.82 is giving us an accurate measurement of the strength of the linear relationship between $x$ and $y$. 

```{r ans2}
anscombe %>%
  filter(set == 1) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

However, in the next set we see something quite different. The relationship here is clearly non-linear. But note also that while the value of the correlation coefficient is 0.82, the correlation---in an intuitive sense---is *perfect*. That is, knowing the value of $x$ tells you the value of $y$ exactly---there is no statistical noise. It's just that the relationship is not linear. 

```{r ans3}
anscombe %>%
  filter(set == 2) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

In the third data set, we have an outlier. Here again, our eyes tell us that with the exception of the outlier, the correlation between $x$ and $y$ is perfect, and in this case, it is linear. However, the presence of the outlier has lowered the value of the correlation coefficient, and - as you will learn later - the slope of the regression line. 


```{r ans4}
anscombe %>%
  filter(set == 3) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

Finally, in the fourth data set we see a pathological example with almost the opposite problem. In the previous plot, we saw that the presence of a single outlier had lowered the correlation from 1 to 0.82. In this plot, the presence of a single outlier raises the correlation from undefined to 0.82. Note that here, with the exception of the outlier, knowing $x$ doesn't tell you anything about $y$. It is only the outlier that gives you the appearance of a correlation, but that correlation is specious. 

```{r ans5}
anscombe %>%
  filter(set == 4) %>%
  ggplot(aes(x = x, y = y)) +
  geom_point()
```

The Anscombe datasets are fabricated, but they help to illustrate some properties of correlation. Above all else, these examples help to underscore the importance of visually inspecting your data! You never know what you are going to see.

Now it's your turn to explore the Anscombe data set. 

### Exploring Anscombe

In 1973, Francis Anscombe famously created four datasets with remarkably similar numerical properties, but obviously different graphic relationships. The `anscombe` dataset contains the `x` and `y` coordinates for these four datasets, along with a grouping variable, `set`, that distinguishes the quartet. 

It may be helpful to remind yourself of the graphic relationship by viewing the four scatterplots:

```{r explot}
ggplot(data = anscombe, aes(x = x, y = y)) +
  geom_point() +
  facet_wrap(~ set)
```


For each of the four `set`s of data points in the `anscombe` dataset, compute the following in the order specified. Names are provided in your call to `summarize()`.

- Number of observations, `N`
- Mean of `x`
- Standard deviation of `x`
- Mean of `y`
- Standard deviation of `y`
- Correlation coefficient between `x` and `y`

```{r ex2, exercise = TRUE}
# Compute properties of anscombe
 anscombe %>%
  group_by(___) %>%
  summarize(
    N = ___, 
    mean_of_x = ___, 
    std_dev_of_x = ___, 
    mean_of_y = ___, 
    std_dev_of_y = ___, 
    correlation_between_x_and_y = ___
  )
```

```{r ex2-hint-1}
 anscombe %>%
  group_by(set) %>%
  ...
```

```{r ex2-hint-2}
 anscombe %>%
  group_by(set) %>%
  summarize(
    N = n(), 
    ...
  )
```

```{r ex2-hint-3}
 anscombe %>%
  group_by(set) %>%
  summarize(
    N = n(), 
    mean_of_x = mean(___), 
    ...
  )
```

```{r ex2-hint-4}
 anscombe %>%
  group_by(set) %>%
  summarize(
    N = n(), 
    mean_of_x = mean(x),
    std_dev_0f_x = sd(___),
    ...
  )
```

```{r ex2-solution}
# Compute properties of anscombe
anscombe %>%
  group_by(set) %>%
  summarize(
    N = n(), 
    mean_of_x = mean(x), 
    std_dev_of_x = sd(x), 
    mean_of_y = mean(y), 
    std_dev_of_y = sd(y), 
    correlation_between_x_and_y = cor(x, y)
  )
```

```{r ex2-check}
grade_result_strict(
  pass_if(~identical(.result$mean_of_x[1], 9)),
  pass_if(~identical(floor(.result$std_dev_of_x[2]),3)),
  pass_if(~identical(floor(.result$mean_of_y[3]),7)),
  pass_if(~identical(floor(.result$std_dev_of_y[2]),2)),
  pass_if(~identical(round(.result$correlation_between_x_and_y[1], digits = 3), 0.816)),
  fail_if(~TRUE, "Not quite,look at the hints for help!")
)
```

### Perception of correlation

Recall the scatterplot between the poverty rate of counties in the United States and the high school graduation rate in those counties from the previous lesson. 

```{r mr3-pre}
ggplot(data = county_complete, 
       aes(x = hs_grad_2010, y = poverty_2010)) + 
  geom_point()
```

```{r mc3}
question("Which of the following values is the correct correlation between poverty rate and high school graduation rate?",
  answer("-0.861", message = "Close, but a correlation of -0.861 seems a bit too strong for the relationship shown on the plot."),
  answer("-0.681", correct = TRUE, message = "The relationship is clearly negative, not too strong, and not too weak..."),
  answer("-0.186", message = "Not quite!"),
  answer("0.186", message = "Incorrect, the slope is not positive!"),
  answer("0.681", message = "Wrong, the relationship isn't positive!"),
  answer("0.861", message = "Try again!"),
  allow_retry = TRUE,
  random_answer_order = TRUE,
  correct = "Correct!"
)
```


Estimating the value of the correlation coefficient between two quantities from their scatterplot can be tricky. [Statisticians have shown that](https://statistics.stanford.edu/sites/default/files/EFS%20NSF%20206.pdf) people's perception of the strength of these relationships can be influenced by design choices like the x and y scales. 

Nevertheless, with some practice your perception of correlation will improve. Toggle through the four scatterplots in the plotting window, each of which you've seen in a previous exercise. Jot down your best estimate of the value of the correlation coefficient between each pair of variables. Then, compare these values to the actual values you compute in this exercise.

If you're having trouble recalling variable names, it may help to preview a dataset in the console with `glimpse()`.


Draw the plot then calculate the correlation between `OBP` and `SLG` for all players in the `mlbbat10` dataset.

```{r ex3-plot, exercise = TRUE}
# Run this and look at the plot
ggplot(data = mlbbat10, aes(x = obp, y = slg)) +
  geom_point()
```

```{r ex3-plot-solution}
# Run this and look at the plot
ggplot(data = mlbbat10, aes(x = obp, y = slg)) +
  geom_point()
```

```{r ex3-plot-check}
grade_code()
```

```{r ex3-cor, exercise = TRUE}
# Correlation for all baseball players
___ %>%
  summarize(N = ___, r = cor(___, ___))
```

```{r ex3-cor-hint-1}
___ %>%
  summarize(N = n(), r = cor(___, ___))
```

```{r ex3-cor-hint-2}
___ %>%
  summarize(N = n(), r = cor(obp, slg))
```

```{r ex3-cor-solution}
# Correlation for all baseball players
mlbbat10 %>%
  summarize(N = n(), r = cor(obp, slg))
```

```{r ex3-cor-check}
grade_result(
  pass_if(~identical(round(.result$r, digits = 1), 0.8), "You have computed the correct correlation!"),
  fail_if(~TRUE, "Make sure you are using the right variable names.")
)
```

Draw the plot then calculate the correlation between `OBP` and `SLG` for all players in the `mlbbat10` dataset with at least 200 at-bats.

```{r ex4-plot, exercise = TRUE}
# Run this and look at the plot
mlbbat10 %>% 
  filter(at_bat > 200) %>%
  ggplot(aes(x = obp, y = slg)) + 
  geom_point()
```

```{r ex4-plot-solution}
# Run this and look at the plot
mlbbat10 %>% 
  filter(at_bat > 200) %>%
  ggplot(aes(x = obp, y = slg)) + 
  geom_point()
```

```{r ex4-plot-check}
grade_code()

```

```{r ex4-cor, exercise = TRUE}
# Correlation for all players with at least 200 ABs
___ %>%
  filter(___) %>%
  summarize(N = ___, r = cor(___, ___))
```

```{r ex4-cor-hint-1}
mlbbat10 %>%
  filter(at_bat ___) %>%
  summarize(N = n(), r = cor(___, ___))
```

```{r ex4-cor-hint-2}
mlbbat10 %>%
  filter(at_bat ___ 200) %>%
  summarize(N = n(), r = cor(obp, ___))
```

```{r ex4-cor-solution}
# Correlation for all players with at least 200 ABs
mlbbat10 %>%
  filter(at_bat >= 200) %>%
  summarize(N = n(), r = cor(obp, slg))
```

```{r ex4-cor-check}
grade_result_strict(
  pass_if(~ identical(round(.result$r, digits = 2), 0.69), "You have computed the correct correlation!"),
  pass_if(~ identical(.result$N, 329)),
  fail_if(~ identical(.result$N, 327), "Did you perhaps use filter(at_bat > 200)? It should be at least 200, hence the inequality should be '>='."),
  fail_if(~TRUE, "Make sure you are using the correct variable names.")
)

```

Draw the plot then calculate the correlation between height and weight for each sex in the `bdims` dataset.

```{r ex5-plot, exercise = TRUE}
# Run this and look at the plot
ggplot(data = bdims, aes(x = hgt, y = wgt, color = factor(sex))) +
  geom_point() 
```

```{r ex5-plot=solution}
# Run this and look at the plot
ggplot(data = bdims, aes(x = hgt, y = wgt, color = factor(sex))) +
  geom_point() 
```

```{r ex5-plot-check}
grade_code()
```

```{r ex5-cor, exercise = TRUE}
# Correlation of body dimensions
___ %>%
  group_by(___) %>%
  summarize(N = ___, r = cor(___, ___))
```

```{r ex5-cor-hint-1}
___ %>%
  group_by(sex) %>%
  summarize(N = ___, r = cor(___, ___))
```

```{r ex5-cor-hint-2}
___ %>%
  group_by(sex) %>%
  summarize(N = n(), r = cor(___, ___))
```

```{r ex5-cor-solution}
# Correlation of body dimensions
bdims %>%
  group_by(sex) %>%
  summarize(N = n(), r = cor(hgt, wgt))
```

```{r ex5-cor-check}
grade_result_strict(
  pass_if(~identical(.result$N[2],247)),
  pass_if(~identical(round(.result$r[1], digits = 2), 0.43), "You have computed the correlation correctly!"),
  fail_if(~identical(.result$N[3],34),"Did you perhaps group by age?"),
  fail_if(~TRUE, "Are you using the correct variable names?")
)

```

Draw the plot then calculate the correlation between body weight and brain weight for all species of `mammals`. Alongside this computation, compute the correlation between the same two quantities after taking their natural logarithms.

```{r ex6-plot, exercise = TRUE}
# Run this and look at the plot
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() + 
  scale_x_log10() + 
  scale_y_log10()
```

```{r ex6-plot-solution}
ggplot(data = mammals, aes(x = BodyWt, y = BrainWt)) +
  geom_point() + 
  scale_x_log10() + 
  scale_y_log10()
```

```{r ex6-plot-check}
grade_code()
```

```{r ex6-cor, exercise = TRUE}
# Correlation among mammals, with and without log
___ %>%
  summarize(N = ___, 
            r = cor(___, ___), 
            r_log = cor(log(___), log(___)))
```

```{r ex6-cor-hint-1}
___ %>%
  summarize(N = n(), 
            r = cor(___, ___), 
            r_log = cor(log(___), log(___)))
```

```{r ex6-cor-hint-2}
___ %>%
  summarize(N = n(), 
            r = cor(body_wt, brain_wt), 
            r_log = cor(log(___), log(___)))
```

```{r ex6-cor-solution}
# Correlation among mammals, with and without log
mammals %>%
  summarize(N = n(), 
            r = cor(body_wt, brain_wt), 
            r_log = cor(log(body_wt), log(brain_wt)))
```

```{r ex6-cor-check}
grade_result(
  pass_if(~identical(round(.result$r_log, digits = 2), 0.96), "You have computed the correlations correctly!"),
  fail_if(~TRUE, "Not quite. Look at the hints for help!"))
)

```

## Interpretation of Correlation

### Exercise and Beer

The word "correlation" has both a precise mathematical definition and a more general definition for typical usage in English. While these uses of the word are obviously related and generally in sync, there are times when these two uses can be conflated and/or misconstrued. This occurs frequently in the media when journalists write about scientific results---particularly in health-related studies. 

For example, consider the following article from the New York Times about the connection between exercise and drinking beer. 

```{r img1, out.width=600}
knitr::include_graphics("images/nytimes_beer.png")
```

After discussing some relatable scenarios, the lead-in raises the question of causation: "But whether exercise encourages people to drink and, likewise, whether drinking encourages people to exercise has been in dispute."

### Exercise and Beer (cont'd)

But causation is not really the goal here. Notice how in the next two paragraphs, verbs linking exercise and beer-drinking include: "influence", "affect", and "tend to". The "interplay between" the two activities is also mentioned. 

```{r img2, out.width=600}
knitr::include_graphics("images/nytimes_beer2.png")
```

In the end, this article is worded fairly carefully. Although many different possible connections between exercising and drinking are explored, the author has taken steps not to imply that the research demonstrates that exercising causes one to drink, nor that drinking causes one to exercise. The best interpretation is quoted from the actual study: "people drank more than usual on the same days that they engaged in more physical activity than usual."

Note how this statement makes it clear that *an association was observed*, but does not try to imply causality. 

In observational studies---which are especially common in nutritional science, health, and epidemiology---one must always be careful not to erroneously suggest that correlation implies causation. 

```{r img3, out.width=600}
knitr::include_graphics("images/nytimes_beer3.png")
```

### NFL arrests

Here is another interesting use of correlation from the New York Times. The article examines the arrest records of National Football League players, and---among other things---considers whether certain franchises tend to have more or fewer players arrested consistently over time. This use of correlation---often called serial correlation or autocorrelation---checks to see whether a single numeric variable is highly correlated with past measurements of itself. 

```{r, include=FALSE, eval=FALSE}
webshot(url = "https://www.nytimes.com/2014/09/13/upshot/what-the-numbers-show-about-nfl-player-arrests.html", 
  file = "images/nytimes_nfl.png", 
  cliprect = c(5550, 0, 820, 600))
```

```{r img4, out.width=600}
knitr::include_graphics("images/nytimes_nfl.png")
```

From the plot and the reported 0.53 correlation, we see some evidence that the number of players arrested from each team during the seven-year period from 2000 to 2006 was positively associated with the number of players arrested by that same team during the subsequent seven-year period from 2007 to 2013. 

### NFL arrests (cont'd)

The author interprets both the plot and reported "pretty solid" correlation as evidence of a team-specific effect. Without offering explanations or implying cause-and-effect, the author effectively communicates that player arrests in the NFL do not appear to be randomly distributed across teams, but rather that certain teams may have player-acquisition philosophies that result in them having consistently higher numbers of players arrested. 


```{r, include=FALSE, eval=FALSE}
webshot(url = "https://www.nytimes.com/2014/09/13/upshot/what-the-numbers-show-about-nfl-player-arrests.html", 
  file = "images/nytimes_nfl2.png", 
  cliprect = c(6150, 0, 820, 500))
```

```{r img5, out.width=600}
knitr::include_graphics("images/nytimes_nfl2.png")
```

### Correlation vs. Regression

Sometimes journalists can leave statistical concepts lost in translation. The first sentence of this article reports that no correlation was found between economic growth and lower tax rates at the top of the income bracket. This is a politically-sensitive finding, and indeed much of the article discusses attempts by Senate Republicans to bury these findings, which run counter to their ideological position on macroeconomic issues. 


```{r, include=FALSE, eval=FALSE}
webshot(url = "http://www.nytimes.com/2012/11/02/business/questions-raised-on-withdrawal-of-congressional-research-services-report-on-tax-rates.html", 
  file = "images/nytimes_tax.png", 
  cliprect = c(400, 0, 820, 400))
```

```{r img6, out.width=600}
knitr::include_graphics("images/nytimes_tax.png")
```

Of greater interest to us is that the word "correlation" here is not being used in its precise statistical sense. The link to the actual report reveals that the findings are based on a multiple regression model, not a simple correlation. Multiple regression models are the subject of the next tutorial, but for now, it is enough to know that multiple regression is a technique that extends regression such that more than one explanatory variable can be taken into account. This is impossible to do with correlation, which is a simple bivariate statistic. 

### Can you plot a correlation?


Finally, it is not uncommon to encounter references to a "plot" of a correlation. This doesn't quite make sense. Of tutorial, a scatterplot may be a graphical presentation of two variables, and the strength of the linear relationship shown can be neatly summarized by a correlation. In this case, the article refers to a table of correlations between several variables - often called a **correlation matrix** - as a "plot" of a correlation. The conclusion that there is no correlation between voting for Brexit and racism is supported by these small correlation values. 

What other evidence would you want to see in order to have confidence in these findings?

```{r, include=FALSE, eval=FALSE}
webshot(url = "http://heatst.com/world/no-correlation-between-voting-for-brexit-and-racism-study-finds/", 
  file = "images/heatst_brexit.png", 
  cliprect = c(1200, 0, 820, 400))
```

```{r img7, out.width=600}
knitr::include_graphics("images/heatst_brexit.png")
```



### Value of Education

[The Value of College: Itâ€™s Not Just Correlation](http://www.nytimes.com/2014/05/28/upshot/the-value-of-college-its-not-just-correlation.html)

The next few exercises will test your ability to interpret correlation like a pro. 

### Interpreting correlation in context

Recall that you previously determined the value of the correlation coefficient between the poverty rate of counties in the United States and the high school graduation rate in those counties was -0.681. 

```{r mc4-pre}
county_complete %>% 
  summarize(cor(hs_grad_2010, poverty_2010))

ggplot(data = county_complete, 
       aes(x = hs_grad_2010, y = poverty_2010)) + 
  geom_point()
```

```{r mc4}
question("Choose the correct interpretation of this value.",
  answer("People who graduate from high school are less likely to be poor.", message = 'Beware the ecological fallacy!' ),
  answer("Counties with lower high school graduation rates are likely to have lower poverty rates.", message = 'Check the sign!'),
  answer("Counties with lower high school graduation rates are likely to have higher poverty rates.", correct = TRUE, message = "Correlation doesn't imply causation, which rules out a couple of the options!"),
  answer("Because the correlation is negative, there is no relationship between poverty rates and high school graduate rates. ", message = 'Negative correlation is still correlation!' ), 
  answer("Having a higher percentage of high school graduates in a county results in that county having lower poverty rates.", message = 'Correlation does not imply causation!'),
  allow_retry = TRUE
)
```


### Correlation and causation

In the San Francisco Bay Area from 1960-1967, the correlation between the birthweight of 1,236 babies and the length of their gestational period was  0.408. 

```{r mc5}
question("Which of the following conclusions **is not** a valid statistical interpretation of these results?",
  answer("We observed that babies with longer gestational periods tended to be heavier at birth."),
  answer("It may be that a longer gestational period contributes to a heavier birthweight among babies, but a randomized, controlled experiment is needed to confirm this observation.", message = "Wrong!"),
  answer("Staying in the womb longer causes babies to be heavier when they are born.", correct = TRUE, message =  "Correlation does not imply causation!"),
  answer("These data suggest that babies with longer gestational periods tend to be heavier at birth, but there are many potential confounding factors that were not taken into account.", message = "Try again!"),
  allow_retry = TRUE
)
```


## Spurious correlations

### Spurious over time

Above all else, we must always remember that correlation does not imply causation. That is, just because two variables appear to move together does not mean that changes in one are causing changes in the other. There are many potential confounders that can cloud our ability to determine cause-and-effect. 

Remarkable - but nonsensical - correlations are called "spurious." In fact, there is an entire blog devoted to them. Here is how the number of films Nicolas Cage has appeared in correlates with the number of people who drowned by falling into a pool.


```{r img8, out.width=600}
knitr::include_graphics("http://tylervigen.com/images/spurious-correlations-share.png")
```

Are these two variables related by any substantive means? Of tutorial not.

### Spurious over time

Here is another example that traces U.S. oil production to the quality of rock music. We suppose one may need to burn the midnight oil to make a truly endearing rock classic! 

```{r img9, out.width=500}
knitr::include_graphics("http://strata.uga.edu/6370/lecturenotes/images/spuriousCorrelationOilMusic.jpg")
```

In each of these cases, the correlation was illustrated as two variables that seem to move together over time. In fact, time is an obvious confounder for many of these spurious correlations. Any time you see two variables linked over time, you should be skeptical of the role that time can play as a confounder. 

### Spurious over space

This webcomic from XKCD illustrates how space can also be present in spurious correlations. Colored maps - called choropleths - can be visually arresting ways of conveying information, but they can also reveal spurious correlations. Many things occur more often in places where there are more people, so failure to control for the confounding influence of population can lead to spurious correlations. 

```{r img10, out.width=500}
knitr::include_graphics("http://imgs.xkcd.com/comics/heatmap.png")
```

### Spurious for whatever reason

In other cases it may not be so obvious what confounding variables are driving the spurious correlation. This plot shows that as the number of lemons imported into the U.S. from Mexico has increased over time, the number of U.S. highway fatalities has decreased. The relationship appears to be strong, but of tutorial, there is no plausible causative mechanism that could explain the relationship. This is most likely simply due to chance. 

The scrupulous statistician must always be on the lookout for such spurious correlations. 

```{r img11, out.width=500}
knitr::include_graphics("http://www.tutor2u.net/_legacy/blog/files//blog-correlation-291209.gif")
```

Now it's time for you to get your hands dirty with some truly spurious correlations.

### Spurious correlation in random data

Statisticians must always be skeptical of potentially spurious correlations. Human beings are very good at seeing patterns in data, sometimes when the patterns themselves are actually just random noise. To illustrate how easy it can be to fall into this trap, we will look for patterns in truly random data.

The `noise` dataset contains 20 sets of `x` and `y` variables drawn at random from a standard normal distribution. Each set, denoted as `z`, has 50 observations of `x`, `y` pairs. Do you see any pairs of variables that might be meaningfully correlated? Are all of the correlation coefficients close to zero?

- Create a faceted scatterplot that shows the relationship between each of the 20 sets of pairs of random variables `x` and `y`. You will need the `facet_wrap()` function for this.
- Compute the actual correlation between each of the 20 sets of pairs of `x` and `y`.
- Identify the datasets that show non-trivial correlation of greater than 0.2 in absolute value.

```{r ex7-setup}
n <- 1000
k <- 20
set.seed(1234)
noise <- data.frame(y = rnorm(n), x = rnorm(n), z = rep(1:k))
```

```{r ex7, exercise = TRUE}
# Create faceted scatterplot

# Compute correlations for each dataset
noise_summary <- noise %>%
  group_by(___) %>%
  summarize(N = ___, spurious_cor = cor(___, ___))

# Isolate sets with correlations above 0.2 in absolute strength
noise_summary %>%
  filter(abs(___) > ___)

```

```{r ex7-hint-1}
ggplot(data = ___, aes(x = x, y = y)) +
  geom_point() + 
  facet_wrap(~ ___)
```

```{r ex7-hint-2}
noise_summary <- noise %>%
  group_by(z) %>%
  summarize(N = n(), spurious_cor = cor(___, ___))
```

```{r ex7-hint-3}
noise_summary %>%
  filter(abs(spurious_cor) > ___)
```

```{r ex7-solution}
# Create faceted scatterplot
ggplot(data = noise, aes(x = x, y = y)) +
  geom_point() + 
  facet_wrap(~ z)

# Compute correlations for each dataset
noise_summary <- noise %>%
  group_by(z) %>%
  summarize(N = n(), spurious_cor = cor(x, y))

# Isolate sets with correlations above 0.2 in absolute strength
noise_summary %>%
  filter(abs(spurious_cor) > 0.2)
```


```{r ex7-check}
grade_result(
  pass_if(~identical(round(.result$spurious_cor[1], digits = 3)), round((noise_summary %>% filter(abs(spurious_cor)>0.2))$spurious_cor[1], digits = 3), "You have computed the correlation correctly!"),
  fail_if(~ TRUE, "Not quite. Look at the hints for help!"))
```

## Congratulations!

You have successfully completed Lesson 2 in Tutorial 3: Introduction to Linear Models.  

What's next?

`r emo::ji("ledger")` [Full list of tutorials supporting OpenIntro::Introduction to Modern Statistics](https://openintrostat.github.io/ims-tutorials/)

`r emo::ji("spiral_notepad")` [Tutorial 3: Introduction to Linear Models Data](https://openintrostat.github.io/ims-tutorials/03-introduction-to-linear-models/)

`r emo::ji("one")` [Tutorial 3 - Lesson 1: Visualizing two variables](https://openintro.shinyapps.io/ims-03-introduction-to-linear-models-01/)

`r emo::ji("two")` [Tutorial 3 - Lesson 2: Correlation](https://openintro.shinyapps.io/ims-03-introduction-to-linear-models-02/)

`r emo::ji("three")` [Tutorial 3 - Lesson 3: Simple linear regression](https://openintro.shinyapps.io/ims-03-introduction-to-linear-models-03/)

`r emo::ji("four")` [Tutorial 3 - Lesson 4: Interpreting regression models](https://openintro.shinyapps.io/ims-03-introduction-to-linear-models-04/)

`r emo::ji("five")` [Tutorial 3 - Lesson 5: Model fit](https://openintro.shinyapps.io/ims-03-introduction-to-linear-models-05/)

`r emo::ji("open_book")` [Learn more at Introduction to Modern Statistics](http://openintro-ims.netlify.app/)
