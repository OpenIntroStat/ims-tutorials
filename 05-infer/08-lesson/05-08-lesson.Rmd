---
title: "Lesson 8 - Comparing many means with ANOVA"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
# load packages ----------------------------------------------------------------
library(learnr)
library(tidyverse)
library(infer)
library(broom)
library(emo)
library(openintro)
library(ggridges)
library(magrittr)

# knitr options ----------------------------------------------------------------

knitr::opts_chunk$set(fig.align = "center", 
                      fig.height = 3, 
                      fig.width = 5,
                      echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE)

# data prep --------------------------------------------------------------------

gss <- read_csv("data/gss_wordsum_class.csv")
```

## Vocabulary score vs. self-identified social class

### Vocabulary score and self-identified social class

So far in this tutorial, we discussed inference on a single mean as well as inference for comparing two means. Next we move on to comparing many means simultaneously.

Our motivating data comes from the General Social Survey. The two variables of interest are vocabulary score and self-identified social class.

Vocabulary score is calculated based on a ten question vocabulary test, where a higher score means better vocabulary, and self-identified social class has 4 levels: lower, working, middle, and upper class.

> - `wordsum`: 10 question vocabulary test (scores range from 0 to 10)
> - `class`: self-identified social class (lower, working, middle, upper)

|   | `wordsum`|`class` |
|:--|:---------|:-------|
|1  |         6|MIDDLE  |
|2  |         9|WORKING |
|3  |         6|WORKING |
|4  |         5|WORKING |
|5  |         6|WORKING |
|6  |         6|WORKING |
|...|       ...|...     |
|795|         9|MIDDLE  |

### Vocabulary score: `wordsum`


> 1. SPACE (school, noon, captain, room, board, don't know)
> 1. BROADEN (efface, make level, elapse, embroider, widen, don't know)
> 3. EMANATE (populate, free, prominent, rival, come, don't know)
> 4. EDIBLE (auspicious, eligible, fit to eat, sagacious, able to speak, don't know)
> 5. ANIMOSITY (hatred, animation, disobedience, diversity, friendship, don't know)
> 6. PACT (puissance, remonstrance, agreement, skillet, pressure, don't know)
> 7. **CLOISTERED (miniature, bunched, arched, malady, secluded, don't know)**
> 8. CAPRICE (value, a star, grimace, whim, inducement, don't know)
> 9. ACCUSTOM (disappoint, customary, encounter, get used to, business, don't know)
> 10. ALLUSION (reference, dream, eulogy, illusion, aria, don't know)


The vocabulary test works as follows: respondents are given the following list of words, and are asked to choose a word from the list that comes closest to the meaning of the first word provided in the capital letters.

For example, is CLOISTERED closest in meaning to miniature, bunched, arched, malady, secluded, or if you were the respondent on this survey would you mark don't know? If you're curious about the vocabulary test feel free to pause and work through the rest, but for the purpose of this example we're not going to be focusing on what these words mean, but instead we'll take a look at how people who took the survey did on the vocabulary test and whether their score is associated with their social class or not.



### Distribution of vocabulary score

```{r echo = TRUE}
ggplot(data = gss, aes(x = wordsum)) +
  geom_histogram(binwidth = 1)
```

The distribution of vocabulary scores is shown in this histogram. The scores range between 0 and 10. The distribution is centered around 6, and looks roughly symmetric. There is a bit of a left skew, but nothing overly dramatic. 

### self-identified social class: `class`


*If you were asked to use one of four names for your social class, which would you say you belong in: the lower class, the working class, the middle class, or the upper class?*
 
```{r echo=TRUE}
ggplot(data = gss, aes(x = class)) +
  geom_bar()
```

And the distribution of social class is shown in this bar plot.

These visualizations tell us about the variables individually, but don't tell us much about their relationship.

Time to put this into practice.

### EDA for vocabulary score vs. social class

Before we conduct inference, we should take a look at the distributions of vocabulary scores across the levels of (self-identified) social class.



- Using `gss`, plot the distribution of vocabulary scores, `wordsum`.
- Make this a histogram, using an appropriate binwidth.
- Facet this histogram, wrapping by social class level.

*Look at the plot! Compare the distributions of vocabulary scores across the levels of (self-identified) social class.*

```{r vocabulary, exercise=TRUE}
# Using gss, plot wordsum
ggplot(data = ___, mapping = aes(___)) +
  # Add a histogram layer
  ___ +
  # Facet by class
  facet_wrap(vars(___))
```

<div id="vocabulary-hint">
**Hint:** 
- Use `gss` as the plot's `data` argument, then map `x` to `wordsum` in c`aes()`. 
- Add a histogram layer with `geom_histogram()`. Vocabulary scores can only be whole numbers, so it doesn't make sense to have a `binwidth` narrower than one (1) point.
- The faceting formula can be specified using `vars(class)`.
</div>

```{r vocabulary-solution}
# Using gss, plot wordsum
ggplot(data = gss, mapping = aes(x = wordsum)) +
  # Add a histogram layer
  geom_histogram(binwidth = 1) +
  # Facet by class
  facet_wrap(vars(class))
```

### 

Great start! Before you move on, make sure you've compared all attributes of the distributions: shape, center, spread, unusual observations.

### Comparing many means, visually

```{r quiz_1}
question("Which of the following plots shows groups with means that are most and least likely to be significantly different from each other?",
  correct = "Correct! The bars in facet `1` look different to each other, so they are more likely to be significantly different to each other.",
  allow_retry = TRUE,
  answer("Most likely: 1, least likely: 2", correct = TRUE),
  answer("Most likely: 1, least likely: 3", message = "No. The bars in plot `2` look more alike than those in facet `3`, so they are less likely to be significantly different to each other."),
  answer("Most likely: 2, least likely: 3", message = "No. The bars in facet `2` look alike, so they are less likely to be significantly different to each other."),
  answer("Most likely: 2, least likely: 1", message = "No. Bars that look different to each other are more likely to be significantly different to each other.")
)
```

```{r means-setup, echo=FALSE}
set.seed(123)

a1 <- rnorm(100, mean = 10, sd = 2)
a2 <- rnorm(100, mean = 20, sd = 2)
a3 <- rnorm(100, mean = 30, sd = 2)
a <- c(a1, a2, a3)

b1 <- rnorm(100, mean = 10, sd = 5)
b2 <- rnorm(100, mean = 11, sd = 5)
b3 <- rnorm(100, mean = 9, sd = 5)
b <- c(b1, b2, b3)

d1 <- rnorm(100, mean = 10, sd = 15)
d2 <- rnorm(100, mean = 20, sd = 15)
d3 <- rnorm(100, mean = 30, sd = 15)
d <- c(d1, d2, d3)

y <- c(a, b, d)
x <- factor(rep(c(rep(1, 100), rep(2, 100), rep(3, 100)), 3))
z <- c(rep("I", 300), rep("II", 300), rep("III", 300))

df <- tibble(x = x, y = y, z = z)

ggplot(df, aes(x = x, y = y)) +
  geom_boxplot() +
  facet_grid( ~ z)
```


## ANOVA


In this lesson we'll formally introduce analysis of variance, in other words ANOVA.

We're going to start our discussion with reviewing the hypotheses for an ANOVA. Next, we'll discuss variability partitioning, considering the different factors that contribute to variability in our response variable.


### ANOVA for vocabulary scores vs. self-identified social class


> $H_0$: The average vocabulary score is the same across all social classes;  
> $\mu_{lower} = \mu_{working} = \mu_{middle} = \mu_{upper}$.
> 
> $H_A$: The average vocabulary score for __*at least one*__ social class differs from the others. 


Let's quickly remind ourselves of the data we're working with from the General Social Survey on vocabulary scores, a numerical variable, and social class, a categorical variable with four levels.

Our null hypothesis is that the average vocabulary score is the same across all social classes, and the alternative hypothesis is that average vocabulary score is different for at least one social class. Notice that the alternative hypothesis __is not__ that the scores for all of the social classes are different! The negation (opposite) of assuming every group is equal is assuming that at least one group is different.


### Variability partitioning

Let's outline this idea of variability partitioning:

The total variability in vocabulary scores times is basically the variance in vocabulary scores of all respondents to the general social survey.

We partition the variability into two sets: 

- Variability that can be attributed to differences in social class, and variability attributed to other factors.

- Variability attributed to social class is called "between group" variability, since social class is the grouping variable in our analysis.

Variability attributed to differences within each social group is called "within group" variability. This variability is not what we are interested in and is somewhat of a nuisance factor. If everyone within a certain social class had the same vocabulary score, then we would have no within group variability and we would be able to more easily compare the vocabulary scores across groups. However, this is almost never the case, and we need to account for the variability within the groups we are interested in. 

> Total variability in vocabulary score:
> 
> - Variability that can be attributed to differences in social class - **between group** variability 
> 
> - Variability attributed to factors within a group - **within group** variability 
> 

## Parametric ANOVA

Here is a look at what the parametric (theoretical) output of an ANOVA model looks like. The first row is about the between group variability, and the second row is about the within group variability. We often refer to the first row as the "group" row, and the second row as the "error" row. Next we'll go through some of the values on the ANOVA table and describe what they mean.


```{r echo=TRUE}
aov(wordsum ~ class, gss) |>
  tidy()
```


### Sum of squares

Let's start with the Sum of Squares column. 

These values measure the variability attributed to the two components: the variability in vocabulary scores explained by social class and the unexplained variability -- that is, unexplained by the explanatory variable in this particular analysis. 

The sum of these two values makes up sum of squares total, which measures the total variability in the response variable, in this case this would be the total variability of the vocabulary scores. 

This value is calculated similarly to the variance, except that it's not scaled it by the sample size. More specifically, this is calculated as the total squared deviation from the mean of the response variable.

One statistic not presented on the ANOVA table that might be of interest is the percentage of the variability in vocabulary scores explained by the social class variable. We can find this as the ratio of the sum of squares for class divided by the total sum of squares.  

> - $SST = 236.5644 + 2869.8003 = 3106.365$ -- Measures the total variability in the response variable 
> - Percentage of explained variability = $\frac{236.5644}{3106.365} = 7.6\%$ 

In this case, 7.6% of the variability in vocabulary scores is explained by self-identified social class. This is the same as the $R^2$ value we would obtain if instead performed a linear regression, explaining vocabulary score with self-identified social class.


### F-distribution

ANOVA uses a test statistic $F$, which represents a standardized ratio of variability in the sample means relative to the variability within the groups. If $H_0$ is true and the model conditions are not violated, the statistic ($F$) follows an $F$-distribution with parameters $df_1 = groups -1$ and $df_2 = n - groups$. 

In the plot below, you see that the $F$-distribution is right skewed! There are no negative values on the $F$-distribution! Thus, for every hypothesis test, **only** the upper tail of the $F$-distribution is used to calculate the p-value.

Similar to the $t$-distribution, the $F$-distribution is defined by degrees of freedom. Except, now there are two different degrees of freedom, the degrees of freedom of the groups and the degrees of freedom of the residuals. 

These degrees of freedom are called the "numerator" and "denominator" degrees of freedom, since they correspond to the mean squares used in the calculation of the $F$-statistic. The "numerator" degrees of freedom is the number of groups you have minus 1 (here: 4 - 1). The "denominator" degrees of freedom is the total number of observations minus the number of groups (here: 795 - 4). 

A plot of the $F$-distribution used to calculate the p-value for this hypothesis test is shown below. 

```{r, echo = FALSE}
values <- data.frame(val=rf(100000, df1 = 3, df2 = 791)) |> 
  tibble()

obs_stat <- aov(wordsum ~ class, gss) |>
  tidy() |> 
  filter(term == "class") |> 
    select(statistic) |> 
  pull()

values |> 
ggplot(aes(x = val)) + 
  geom_density() + 
  xlim(c(0, 25)) + 
  labs(x = "F-statistic", 
       y = "Density", 
       title = "F-distribution with 3 and 791 Degrees of Freedom")

```


### F-statistic

The $F$-statistic is calculated as the ratio between the “between” and “within” group variabilities, assuming all groups means were equal. Here when we talk about "variability" we are scaling the sum of squares for each group by its degrees of freedom. This gets us to the mean square values seen in the table. The ratio of these mean squares is how the $F$-statistic is calculated. 

The p-value is the area under the $F$-distribution beyond the observed $F$-statistic. We draw conclusions based on this p-value just like with any other hypothesis test we've seen so far.


```{r}
aov(wordsum ~ class, gss) |>
  tidy()
``` 

> F-statistic = $\frac{\frac{between~group~var}{group~size}}{\frac{within~group~var}{n - group~size}} = \frac{\frac{236.5644}{3}}{\frac{2869.8003}{791}} = \frac{78.854810}{3.628066} = 21.73467$

</br> 

The same $F$-distribution as above is shown below, with the observed $F$-statistic is displayed in red. 

```{r, echo = FALSE}
values <- data.frame(val=rf(100000, df1 = 3, df2 = 791)) |> 
  tibble()

obs_stat <- aov(wordsum ~ class, gss) |>
  tidy() |> 
  filter(term == "class") |> 
    select(statistic) |> 
  pull()

values |> 
ggplot(aes(x = val)) + 
  geom_density() + 
  xlim(c(0, 25)) + 
  labs(x = "F-statistic", 
       y = "Density", 
       title = "F-distribution with 3 (numerator) and 791 (denominator) \n Degrees of Freedom") + 
  geom_vline(xintercept = obs_stat, color = "red", linetype = "dashed")

```

Time to put this into practice.

## Your turn! 

__ANOVA for evaluation score vs. rank of professor__

Let's conduct the ANOVA for evaluating whether there is a difference in the average evaluation score between the different ranks of professors at the University of Texas at Austin.

Use the `evals` data to perform an ANOVA for evaluation `score` based on professor `rank`. Use the following steps: 

- Use the `aov()` to perform the ANVOA, with inputs `score` and `rank`. 
- Store the resulting object as `aov_evals_rank`.
- View a `tidy()` output of this object.


```{r vocabulary_2, exercise=TRUE}
# Run an analysis of variance on score vs. rank
aov_evals_rank <- aov(___, data = ___)

# Tidy the model
tidy(aov_evals_rank)
```

<div id="vocabulary_2-hint">
**Hint:** 
- Call `aov()`, passing a formula of `score ~ rank`, using the `evals` dataset.
- Call `tidy()`, passing the `aov_evals_rank` AOV model.
</div>


```{r vocabulary_2-solution}
# Run an analysis of variance on score vs. rank
aov_evals_rank <- aov(score ~ rank, data = evals)

# Tidy the model
tidy(aov_evals_rank)

```

Interpret the result in context of the data and the research question. If needed, use a 2.5% significance level.  

Would your conclusion change if you had used a 10% significance level?


## Conditions for ANOVA

Just like any other statistical inference method we've encounter so far, there are mathematical conditions that need to be met for an ANOVA as well. Since we cannot mathematically "prove" that these conditions have been "met," we will use a careful eye to evaluate the degree to which each condition may be violated. 

There are three main conditions for ANOVA. The first one is independence. Within groups the sampled observations must be independent of each other, and between groups the groups must be independent of each other as well. 

We also need approximate normality, that is the distributions within each group should be nearly normal. 

Finally, we have the condition of constant variance. That is the variability of the distributions of the response variable within each group should have roughly the same variance.

> - **Independence:**
>     - within groups: sampled observations must be independent 
>     - between groups: the groups must be independent of each other
> - **Approximate normality:** distribution of the response variable should be nearly normal within each group
> - **Equal variance:** groups should have roughly equal variability

Next we'll discuss each condition in more detail.

### Independence

Let's start with the independence condition.

Within groups we want the samples observations to be independent. We can assume this is the case if we have random sampling. For experiments with random assignment but without random sampling, researchers would need to carefully consider if any of the observations could be related. For studies with small sample sizes, we need to be sure that the each sample size is less than 10% of its respective population. This condition is always important, but can be difficult to check if we don't have sufficient information on how the study was designed and data were collected.

Between groups we want the groups to be independent of each other. This requires carefully considering whether there is a paired structure between the groups. If the answer is yes, this is not the end of the world, but it requires a different, slightly more advanced version of ANOVA. 

> - **Within groups:** Sampled observations must be independent of each other
>     - Random sample 
>     - Each $n_j$ less than 10% of respective population always important, but sometimes difficult to check
>      
> - **Between groups:** Groups must be independent of each other  
>     - Carefully consider whether the groups may be dependent 
>     - Cannot have paired (or repeated) obervations in the groups


### Approximately normal

We also need the distribution of the response variable within each group to be approximately Normal. This condition is especially important when the sample sizes are small! We can check this condition using appropriate visualizations, which you'll get to do in the following exercises.

### Constant variance

Lastly we need constant variance across groups, in other words variability should be consistent across groups. This condition is especially important when the sample sizes differ between groups! We can use visualizations and/or summary statistics to check this condition.

Next we'll check the conditions for the vocabulary score vs. social class ANOVA that we have been working on.

### Checking the normality condition

```{r quiz_2}
question("Which of the following provides the most complete information for checking the normality condition for the ANOVA for evaluating whether there are differences between the average vocabulary scores across social classes?",
  correct = "Correct! A violin plot shows you the shape of the distribution.",
  allow_retry = TRUE,
  answer("Violin plot of vocabulary scores, faceted by social class", correct = TRUE),
  answer("Box plot of vocabulary scores, faceted by social class", message = "No. A box plot only gives you 5 metrics about the distribution of a variable, plus the positions of the outliers."),
  answer("Means and standard deviations of vocabulary scores in each social class", message = "No. This only gives you two metrics about the distribution of the scores, but doesn't tell you about the shape of the distribution."),
  answer("Number of modes of vocabulary scores in each social class", message = "No. This only gives you details of the modality of the distribution of the scores, but doesn't show you the shape of the distribution.")
)
```

### Checking the constant variance condition

In addition to checking the normality of distributions of vocabulary scores across levels of social class, we need to check that the variances from each are roughly constant.

In the exercise below, you'll calculate the standard deviations of each social class. To do this: 

1. `group_by()` social `class` 
2 `summarize()` each group with the standard deviations (`sd()` ) of vocabulary scores (`wordsum`)
3. Store these summaries in a column named `sd_wordsum`. 

Use these calculations to decide if it seems reasonable to assume that the groups have equal variability. 


```{r vocabulary_3, exercise=TRUE}
gss |>
  # Group by class
  group_by(___) |>
  # Calculate the std dev of wordsum as std_dev_wordsum
  summarize(___ = ___)
```

<div id="vocabulary_3-hint">
**Hint:** 
- Call `group_by()`, passing `class` as the argument.
- Call `summarize()`, setting `std_dev_wordsum` to `sd(wordsum)`.
</div>

```{r vocabulary_3-solution}
gss |>
  # Group by class
  group_by(class) |>
  # Calculate the std dev of wordsum as std_dev_wordsum
  summarize(sd_wordsum = sd(wordsum))
```

### 

So, what do you think? Is the equal variance condition violated? 

Let's see what these standard deviations *look* like. 

Create ridge plots of the distribution of `wordsum` across the different self-identified social `class`es.  

```{r vocabulary_4, exercise=TRUE}
gss |>
  # Map wordsum to the x-axis and class to the y-axis
  ggplot(aes(x = ___, y = ___)) +
  # Add density ridges to the plot! 
  geom_density_ridges()
```

<div id="vocabulary_4-hint">
**Hint:** 
- Call `ggplot()`, passing `wordsum` to the `x` variable and `class` to the `y` variable. 
</div>

```{r vocabulary_4-solution}
gss |>
  # Map wordsum to the x-axis and class to the y-axis
  ggplot(aes(x = wordsum, y = class)) +
  # Add density ridges to the plot! 
  geom_density_ridges()
```

So, what do you think now? Is the equal variance condition violated? 

## Simulation-based ANOVA

If the condition of normality is violated, then the $F$-distribution is a poor approximation for what the true sampling distribution of the F-statistics looks like. Instead, we should use a more robust method. The familiar randomization and bootstrapping methods from before can be extended into the ANOVA framework. We really only need to make one modification:

> When we shuffle our observations together and make new groups, now we will have more than two groups to divide the observations into. 
All of the other steps for creating the null distribution are the same! Let's see how this is done. 

### Calculating the observed statistic

In the case of an ANOVA, the statistic we are interested in is the F-statistic. While we can plot this statistic on an $F$-distribution, is really is just another statistic that we can calculate (like the mean or median). We like this statistic because it allows for us to summarize how different multiple means are from each other, relative to how variable the observations are within each group. 

We can `calculate` the F-statistic using the tools from the infer package we are familiar with. The only part that is new is the `stat` that we calculate. Here, we use the `"F"` statistic. This is what this looks like: 

```{r, echo = TRUE}
obs_stat <- gss |> 
  specify(wordsum ~ class) |> 
  calculate(stat = "F")
```

The observed statistic for these data is an F-statistic equal to `r pull(obs_stat)`. Let's see how extreme this is, if there really is no difference in the `wordsum` between the four groups. 

### Simulating samples under the null hypothesis

The next step is to simulate what we would expect for `wordsum`s to look like, if the null hypothesis was true. This is similar to the method for a difference in means, except now we have four groups: lower, middle, upper, and working. The underlying process, however, looks the same:

- Step 1: Write the values of `wordsum` on 795 index cards (one card per person).
- Step 2: Shuffle the cards and randomly split them into four new piles, of the same size as the original groups.
- Step 3: Calculate and record the test statistic: F-statistic
- Step 4: Repeat steps (1) and (2) many times to generate the sampling distribution of the difference in means under the null hypothesis.
- Step 5: Calculate p-value as the percentage of simulations where the test statistic is at least as extreme as the observed F-statistic

###

Let's start with steps 1-4. Using the code below, fill in the missing pieces necessary to simulate F-statistics that could have happened if the null hypothesis was true. 

Once you've filled in the code, take a look at what the F-statistics stored inside of `null_dist` look like!

```{r anova-sim, exercise = TRUE}
null_dist <- gss |> 
  specify(___ ~ ___) |> 
  hypothesize(null = "___") |> 
  generate(reps = 1000, type = "permute") |> 
  calculate(stat = "___")

```

```{r anova-sim-hint-1}
specify(wordsum ~ class) 
```

```{r anova-sim-hint-2}
hypothesise(null = "independence")
```

```{r anova-sim-hint-3}
calculate(stat = "F")
```

###

Now that we have our observed F-statistic and our null distribution, we can calculate our p-value! Let's visualize where the observed statistic lies on the null distribution first. 

```{r, include = FALSE}
null_dist <- gss |> 
  specify(wordsum ~ class) |> 
  hypothesize(null = "independence") |> 
  generate(reps = 1000, type = "permute") |> 
  calculate(stat = "F")
```

```{r, echo = TRUE}
null_dist |> 
  visualise() +
  shade_p_value(obs_stat = obs_stat, direction = "greater")
```

Woah! That's a long ways away from the null distribution! As you would suspect, there are no permuted samples with F-statistics as or more extreme that what we observed in the dataset. Thus, our p-value is approximately 0. 

Similar to before, with this p-value we can conclude that *at least one* of the classes has a different mean `wordsum`. But, you might wonder, which of the classes are different? That's where post-hoc testing picks up! 


## Post-hoc testing

So far we've introduced ANOVA as a method for comparing many means to each other concurrently. Finding a statistically significant result at the end of an ANOVA however only tells us that at least one pair of means are different, but not which pair of means are different. Next, we set out to answer this follow up question.

### Which means differ?

What we're talking about is performing lots and lots of $t$-test, testing which of the groups are different. Unfortunately, performing lots and lots of hypothesis test has some major downsides. The main one we are worried about is the inflation in the Type I error rate. 
If you remember back to the [Errors in hypothesis testing tutorial](https://openintro.shinyapps.io/ims-04-foundations-03), a Type I error is when we reject the null hypothesis when it is true. We specify a threshold for the percentage of times we are willing to make this type of error by selecting an $\alpha$. So, an $\alpha$ of 0.05, says we are willing to make a Type I error 5% of the time. 

If we think about performing multiple $t$-tests all at a 5% significance level, then our error rate begins to grow. Specifically, if we perform 10 tests all with an $\alpha$ of 0.05, the probability of not making a Type I error for each test is $(1- \alpha)$. If we then compute the overall probability of not making a Type I error we'd have $1 - (1- \alpha)^n \approx n \cdot \alpha.$ For 10 tests at a $\alpha$ of 0.05, the probability of not making a Type I error is approximately 50%. That's not very unlikely!  

So, what we're interested in is controlling the Type I error rate, when performing many pairwise tests in the quest for identifying the groups whose means are "significantly" different from each other.

Fortunately, there is a simple solution to this problem: use a modified significance level. 

That is, we'll use a "family" error rate, and then distribute that level to each of the tests we are performing. The "family" error rate specifies an overall Type I error rate we are willing to have for **all** of tests you wish to perform. 


### Multiple comparisons

Testing many pairs of groups is called multiple comparisons. 

A common modification we use when doing multiple comparisons is the Bonferroni correction. This correction fixes a "family" error rate which then transfers to a more stringent significance level for each of the pairwise tests. 

More specifically, we will adjust our "family" $\alpha$ by the number of comparisons we are doing. 

The Bonferroni corrected significance level can be calculated as the original significance level ($\alpha$) divided by the number of pairwise comparisons to be carried out. 

If we think back to our days in Algebra, we can calculate the total number of tests using combinatorics. Specifically, for $k$ groups we will have ${k \choose 2}$ possible pairwise tests. This number of tests can be calculated as $\frac{k \cdot (k - 1)}{2}$, where k is the number of groups in the ANOVA.

> - Testing many different pairs of groups is called multiple comparisons
> - Due to the inflated Type I error rate, a correction for the significance level is needed 
>     - The Bonferroni correction uses a more stringent significance level  
>     - It adjusts $\alpha$ by the number of comparisons being considered 
>     - The new $\alpha$ is $\alpha^\star = \frac{\alpha}{K}$, where $K = \frac{k (k-1)}{2}$ 



Now it's your turn.

### Calculate $\alpha^*$

Which of the following is the correct modified significance value for the post hoc tests associated with ANOVA for evaluating which of the self-identified social classes have different average vocabulary scores?

There are 4 social classes, and the original significance level was 5%.

*Hint:* For $k$ groups, there are $k \cdot (k - 1) / 2$ pairwise comparisons.

```{r quiz_3}
question("",
  correct = "Bonferroni would be so proud! The correction factor for $k$ classes is $k * (k - 1) / 2$.",
  allow_retry = TRUE,
  answer("0.05", message = "No. You need to correct for the multiple comparisons of pairs of social classes."),
  answer("0.05 / 4", message = "No. You should correct for *pairwise* comparisons of social classes."),
  answer("0.05 / 6", correct = TRUE),
  answer("0.05 / 12", message = "No. Comparing class `A` to class `B` is the same as comparing class `B` to class `A`, so you need to halve the correction factor.")
)
```

### Compare pairwise means

We can compute all of the pairwise $t$-tests for our 4 social classes using the `pairwise.t.test()` function. This function is a bit different than other functions we have used, in that it expects for the variables to be extracted from the dataframe when they are input. This is different from our "usual" workflow where we "pipe" data from the dataset into the function. 

There are two ways to handle this,  

1. Extract the columns you are interested in using a `$` and use as the inputs to the `pairwise.t.test()` function.  
2. Use a modified pipe operator to extract the columns (`%$%`), when piping the data into the `pairwise.t.test()` function. 

We will present the modified pipe operator option, but both methods will be displayed in the solution. 

Additionally, the `pairwise.t.test()` function takes three main inputs,   

1. the response variable (input as `x`)
2. the grouping variable (input as `g`)
3. the adjustment method to use (input as `p.adjust.method`) -- there are other adjustment methods, but in this exercise we will focus on the `"bonferroni"` adjustment 

### 

For this exercise, you are to conduct a pairwise t-test on vocabulary scores and social class.

- Use the `pairwise.t.test()` function to obtain the pairwise $t$-tests. 
- Set the `p.adjust.method` to `"Bonferroni"` to obtain the Bonferroni corrected p-values.
- Tidy the resulting table. 
- Compare the adjusted p-values to the un-adjusted p-values. Which p-values are smaller? 


```{r vocabulary_5, exercise=TRUE}
# Run a pairwise t-test on wordsum and class, without adjustment
t_test_results <- gss %$%  
  pairwise.t.test(x = ___, g = ___, p.adjust.method = "___")


# Tidy the result
tidy(___)

## Compare with un-adjusted pairwise t-tests
gss %$% 
  pairwise.t.test(wordsum, class, p.adjust.method = "none")
```

<div id="vocabulary_4-hint">
**Hint:** 
- Use `wordsum` as `x`, `class` as `g`, and set `p.adjust.method` to `"bonferroni"`.
- Call `tidy()`, passing `t_test_results`.
</div>

```{r vocabulary_5-solution}
# Run a pairwise t-test on wordsum and class, without adjustment
t_test_results <- gss %$%  
  pairwise.t.test(x = wordsum, g = class, p.adjust.method = "bonferroni")

## Alternative method 
t_test_results <- pairwise.t.test(gss$wordsum, gss$class, p.adjust.method = "bonferroni")

# Tidy the result
tidy(t_test_results)

## Compare with un-adjusted pairwise t-tests
gss %$% 
  pairwise.t.test(wordsum, class, p.adjust.method = "none") |> 
  tidy()

```

### 


Do the data provide convincing evidence of a difference in the average vocabulary scores of those who self-identified as middle class and those who self-identified as lower class?

For which of the pairwise comparisons would you conclude that there is a "significant" difference between the group means?

## Congratulations!

You have successfully completed Lesson 8 in Tutorial 5: Statistical inference.  

You should now have a very good understanding of statistical inference for numerical data. In this tutorial you have learned about parametric procedures for conducing an analysis of variance (ANOVA). Specifically, you should now be familiar with the concepts of variability within and between groups, the $F$-distribution, and multiple comparison procedures. 

What's next?

`r emo::ji("ledger")` [Full list of tutorials supporting OpenIntro::Introduction to Modern Statistics](https://openintrostat.github.io/ims-tutorials/)

`r emo::ji("spiral_notepad")` [Tutorial 5: Statistical inference](https://openintrostat.github.io/ims-tutorials/05-infer/)

`r emo::ji("one")` [Tutorial 5 - Lesson 1: Inference for a single proportion](https://openintro.shinyapps.io/ims-05-infer-01/)

`r emo::ji("two")` [Tutorial 5 - Lesson 2: Hypothesis Tests to Compare Proportions](https://openintro.shinyapps.io/ims-05-infer-02/)

`r emo::ji("three")` [Tutorial 5 - Lesson 3: Chi-squared Test of Independence](https://openintro.shinyapps.io/ims-05-infer-03/)

`r emo::ji("four")` [Tutorial 5 - Lesson 4: Chi-squared Goodness of Fit Test](https://openintro.shinyapps.io/ims-05-infer-04/)

`r emo::ji("five")` [Tutorial 5 - Lesson 5: Bootstrapping for estimating a parameter](https://openintro.shinyapps.io/ims-05-infer-05/)

`r emo::ji("six")` [Tutorial 5 - Lesson 6: Introducing the t-distribution](https://openintro.shinyapps.io/ims-05-infer-06/)

`r emo::ji("seven")` [Tutorial 5 - Lesson 7: Inference for difference in two parameters](https://openintro.shinyapps.io/ims-05-infer-07/)

`r emo::ji("eight")` [Tutorial 5 - Lesson 8: Comparing many means](https://openintro.shinyapps.io/ims-05-infer-08/)

`r emo::ji("open_book")` [Learn more at Introduction to Modern Statistics](http://openintro-ims.netlify.app/)
