---
title: "Regression modeling: 10 - Case Study: Italian restaurants in NYC"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
# load packages ----------------------------------------------------------------

library(learnr)
library(openintro)
library(Stat2Data)
library(tidyverse)
library(emo)
library(modelr)
library(plotly)
library(broom)

# knitr options ----------------------------------------------------------------

knitr::opts_chunk$set(fig.align = "center", 
                      fig.height = 3, 
                      fig.width = 5,
                      echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE)

# data prep --------------------------------------------------------------------

# https://gattonweb.uky.edu/sheather/book/docs/datasets/nyc.csv
nyc <- read_csv("data/nyc.csv")
```


## Case Study: Italian restaurants in NYC

Explore the relationship between price and the quality of food, service, and decor for Italian restaurants in NYC.

No matter where you are, or where you've been, if you're taking this tutorial, my guess is that you've been to a restaurant before. And if you're in Europe or North America, there's an excellent chance that you've been to an Italian restaurant. The Italian restaurants in New York City are legendary, and it's time to put your newly-developed regression modeling skills to work to understand how they operate. 

What are the factors that contribute to a price of a meal at Italian restaurants in New York City? We will address this question with a series of multiple regression models.

```{r italian-restaurants, out.width = '60%'}
# https://lh3.googleusercontent.com/q1Omz8yShaJS6GfVirMfYHAD-QkjWcEe8yqNGQuh01R97P8FVPP1_0voo7XnpmM-=s1307
knitr::include_graphics("images/italian-restaurants.jpg")
```



### Zagat guide

The Zagat guide is an influential review of restaurants. They are made famous by their use of selective quotes from actual diners to build a pithy description of each restaurant. But we won't be working with the text. Rather, we'll be working with the numeric reviews you see posted here. Each restaurant is rated on a scale of 0 to 30 for the quality of its food, decor, and service. The average price of a meal---which includes one drink and a tip---will be our response variable. How do these factors influence the average price of a meal?

```{r zagat, out.width = '30%'}
# http://www.lamasserianyc.com/new-york/images/zagat.jpg
knitr::include_graphics("images/zagat.jpg")
```






### Exploring the data


```{r echo=TRUE}
glimpse(nyc)
```




Our data come in the form of Zagat reviews from 168 Italian restaurants in New York City from 2001. The `glimpse()` function can help you get a sense of how the variables are encoded and what values they typically take on. The `East` variable records whether the restaurant is located east or west of Fifth Avenue, which historically divides the island of Manhattan.


### EDA


```{r nyc-pairs, echo=TRUE}
nyc |>
  select(-Restaurant) |>
  pairs()
```


Before we can sensibly build models, we should spend some time exploring the data. How are the variables distributed? How are they related to one another? What patterns are present? The `pairs()` function can help us explore these relationships visually. It shows a grid of scatterplots for each pair of variables in the `nyc` data frame. It's easy to see that, for example, `Price` and `Decor` are strongly correlated, while `Case` and `Food` are uncorrelated.

### Exploratory data analysis

Multiple regression can be an effective technique for understanding how a response variable changes as a result of changes to more than one explanatory variable. But it is not magic -- understanding the relationships among the explanatory variables is also necessary, and will help us build a better model. This process is often called **exploratory data analysis** (EDA) and is covered in the previous tutorial on Summarizing and Visualizing Data.

One quick technique for jump-starting EDA is to examine all of the pairwise scatterplots in your data. This can be achieved using the `pairs()` function. Look for variables in the `nyc` data set that are strongly correlated, as those relationships will help us check for **multicollinearity** later on.  


```{r ex1, exercise=TRUE}

```

<div id="ex1-hint">
**Hint:** The `paris()` function is helpful for looking for pairs of variables that have high correlation.
</div>

```{r ex1-solution}
pairs(nyc)
```

```{r mc1}
question("Which pairs of variables appear to be strongly correlated?",
  answer("`Case` and `Decor`.", message = "Is that a strong relationship?"),
  answer("`Restaurant` and `Price`.", message = "What does restaurant mean?"),
  answer("`Price` and `Food`.", correct = TRUE),
  answer("`Price` and `East`.", message = "Do you see a linear pattern?"),
  allow_retry=TRUE
)
```


### SLR models


Based on your knowledge of the restaurant industry, do you think that the quality of the food in a restaurant is an important determinant of the price of a meal at that restaurant? It would be hard to imagine that it wasn't. We'll start our modeling process by plotting and fitting a model for `Price` as a function of `Food`. 

On your own, interpret these coefficients and examine the fit of the model. What does the coefficient of `Food` mean in plain English? "Each additional rating point of food quality is associated with a..."


- Use `ggplot` to make a scatter plot for `Price` as a function of `Food`. 
- Use `lm()` to fit a simple linear regression model for `Price` as a function of `Food`.



```{r ex2, exercise=TRUE}
# Price by Food plot


# Price by Food model

```

```{r ex2-hint-1}
ggplot(data = nyc, aes(x = Food, y = Price)) +
  geom_point()
```

```{r ex2-hint-2}
lm(Price ~ Food, data = nyc)
```

```{r ex2-solution}
# Price by Food plot
ggplot(data = nyc, aes(x = Food, y = Price)) +
  geom_point()

# Price by Food model
lm(Price ~ Food, data = nyc)
```

## Incorporating another variable



### Fifth Avenue

Fifth Avenue is one the most well-known streets in the world, renowned for its flagship stores for shopping, landmark hotels, and internationally-recognized sites. A walk down 5th Avenue from Central Park to Washington Square Park would take you past the Plaza Hotel, Trump Tower, Saks Fifth Avenue, the New York Public Library, the Empire State Building, and the Flatiron building.

```{r 5th-avenue, out.width = '60%'}
# https://upload.wikimedia.org/wikipedia/commons/0/0e/Photograph_of_Fifth_Avenue_from_the_Metropolitan%E2%80%94New_York_City.jpg
knitr::include_graphics("images/5th-avenue.jpg")
```

### CUNY

It would also take you past the Graduate Center of the City University of New York, where one of the authors studied for their doctoral degree. Good times!

```{r cuny, out.width = '60%'}
# https://upload.wikimedia.org/wikipedia/commons/b/b6/CUNY_Graduate_Center_by_David_Shankbone.jpg
knitr::include_graphics("images/cuny.jpg")
```




### Dividing the City


5th Avenue divides the island of Manhattan vertically. The city's east side has historically been home to expensive residences and opulent attractions. Maybe everything is more expensive on the East side. Maybe even food is more expensive on the east side. Do Italian restaurants located on the east side of 5th Avenue tend to charge more? How much more?


```{r dividing-city, out.width = '60%'}
# https://s-media-cache-ak0.pinimg.com/originals/0e/1b/aa/0e1baaa971bf23b610077ff05d43b79c.jpg
knitr::include_graphics("images/dividing-city.jpg")
```



### The price of location


```{r echo=TRUE}
nyc |> 
  group_by(East) |> 
  summarize(mean_price = mean(Price))
```



It's certainly true that the average price of a meal for restaurants located on the east side of Manhattan was higher than it was for restaurants on the west side. The table above shows that they charged about $3.58 more, on average. But does this figure really represent the premium for being on the east side? Could it be the case that the restaurants on the east side also had better food, and that is what was driving the increase in price? You'll explore this question in the exercises.



### Service

Another important consideration in dining out is the quality of the service. Are people willing to pay more for higher quality service, even if the food doesn't taste any better? How much more? How does the average price of meal vary with respect to the quality of the food and the service? It's up to you to figure it out.


```{r service, out.width = '60%'}
# http://www.coylehospitality.com//wp-content/uploads/2011/01/3114585_thumbnail.jpg
knitr::include_graphics("images/service.jpg")
```

### Parallel lines with location

In real estate, a common mantra is that the three most important factors in determining the price of a property are "location, location, and location." If location drives up property values and rents, then we might imagine that location would increase a restaurant's costs, which would result in them having higher prices. In many parts of New York, the east side (east of 5th Avenue) is more developed and perhaps more expensive. [This is increasingly less true, but was more true at the time these data were collected.]

Let's expand our model into a parallel slopes model by including the `East` variable in addition to `Food`. 

* Use `lm()` to fit a parallel slopes model for `Price` as a function of `Food` and `East`. 
* Interpret the coefficients and the fit of the model. 
* Can you explain the meaning of the coefficient on `East` in simple terms? Did the coefficient on `Food` change from the previous model? If so, why? Did it change by a lot or just a little? 


```{r ex3, exercise=TRUE}

```

<div id="ex3-hint">
*Hint:* What is the difference between the coefficients that are in both models?
</div>

```{r ex3-solution}
lm(Price ~ Food + East, data = nyc)
```

### 

```{r mc2}
question("Identify the statement that is *FALSE*:",
  answer("Each additional rating point of food quality is associated with a $2.88 increase in the expected price of meal, after controlling for location.", correct = TRUE),
  answer("The premium for an Italian restaurant in NYC associated with being on the east side of 5th Avenue is $1.46, after controlling for the quality of the food.
", correct = TRUE),
  answer("The change in the coefficient of food from $2.94 in the simple linear model to $2.88 in this model has profound practical implications for restaurant owners.", correct = TRUE),
allow_retry=TRUE
)
```







### A plane in 3D


One reason that many people go to a restaurant - apart from the food - is that they don't have to cook or clean up. Many people appreciate the experience of being waited upon, and we can all agree that the quality of the service at restaurants varies widely. Are people willing to pay more for better restaurant `Service`? More interestingly, are they willing to pay more for better service, after controlling for the quality of the food? 

Multiple regression gives us a way to reason about these questions. Fit the model with `Food` and `Service` and interpret the coefficients and fit. Did the coefficient on `Food` change from the previous model? What do the coefficients on `Food` and `Service` tell you about how these restaurants set prices? 

Next, let's visually assess our model using `plotly`. The `x` and `y` vectors, as well as the `plane` matrix, have been created for you.



- Use `lm()` to fit a multiple regression model for `Price` as a function of `Food` and `Service`.
- Use `plot_ly` to draw 3D scatterplot for `Price` as a function of `Food` and `Service` by mapping the `z` variable to the response and the `x` and `y` variables to the explanatory variables. Place the food quality on the x-axis and service rating on the y-axis.
- Use `add_surface()` to draw a plane through the cloud of points using the object `plane`.

```{r ex4-setup}
mod <- lm(Price ~ Food + Service, data = nyc)

grid <- nyc |>
  modelr::data_grid(
    Food = seq_range(Food, n = 50),
    Service = seq_range(Service, n = 50)
  )

tidy_planes <- mod |>
  augment(newdata = grid)

x <- unique(grid$Food)
y <- unique(grid$Service)

plane <- tidy_planes |>
  pull(.fitted) |>
  matrix(nrow = length(x), byrow = TRUE)
```

```{r ex4, exercise=TRUE}
# fit model


# draw 3D scatterplot
p <- plot_ly(data = nyc, z = ~___, x = ~___, y = ~___, opacity = 0.6) |>
  add_markers() 

# draw a plane
p |>
  add_surface(x = ~x, y = ~y, z = ~___, showscale = FALSE) 
```

<div id="ex4-hint">
**Hint:** Remember `add_surface()`?
</div>

```{r ex4-solution}
# fit model
lm(Price ~ Food + Service, data = nyc)

# draw 3D scatterplot
p <- plot_ly(data = nyc, z = ~Price, x = ~Food, y = ~Service, opacity = 0.6) |>
  add_markers() 

# draw a plane
p |>
  add_surface(x = ~x, y = ~y, z = ~plane, showscale = FALSE) 
```




## Higher dimensions



### Decor

The last criteria for the Zagat review is decor. This refers to the visual presentation of the restaurant: the furniture, the wall adornments, and the overall ambiance. Clearly, restaurants vary widely in the quality of their decor. But how does the perceived quality of decor vary with the price of a meal? And how is the effect moderated by the quality of food and service?

```{r pizza-restaurant, out.width = '60%'}
# https://static.pexels.com/photos/3498/italian-pizza-restaurant-italy.jpg
knitr::include_graphics("images/italian-pizza-restaurant-italy.jpg")
```



### Building a full model

In what follows, you will build more complex models for the price of a meal at an Italian restaurant in New York City using the Zagat ratings. The possible explanatory variables include `Food`, `Service`, `Decor`, and `East`. Other variables recorded in the data set that should not be considered explanatory variables are `Case` (which records an ID number for each restaurant), and `Restaurant` (which records the name of the restaurant). 

- Response variable:
  - `Price`
- Explanatory variables
    - `Food`
    - `Service`
    - `Decor`
    - `East` (categorical)
- Unusable
  - `Case`
  - `Restaurant`


Consider for a moment what a full model that included all of these explanatory variables would look like.



### Collinearity


```{r echo=TRUE}
nyc |>
  mutate(Price_cents = Price / 100) |>
  summarize(cor_collinear = cor(Price, Price_cents))
```

If one variable is a constant multiple of another variable, then they are said to be collinear. If two collinear variables are explanatory variables in a model, the second one won't tell you anything, because it is providing redundant information. A solution would be to simply drop one of the variables from the model. Which one you drop doesn't matter, since they are providing the same information. 

Here we demonstrate collinearity between Price measured in dollars and Price measured in cents. The values of the two variables aren't the same, but they are perfectly correlated. If you already know the price in dollars, you don't get any additional information from knowing the price in cents. 

Real data hardly ever lines up perfectly, so you'll rarely have perfect collinearity. However, variables that are highly correlated will be approximately collinear. Furthermore, the collinearity does not have to be one-to-one. It might be the case that one variable is collinear with a linear combination of several other variables. This can make it much harder to figure out which variable might be appropriate to drop.



### Multicollinearity


- Explanatory variables are highly correlated
- Unstable coefficient estimates 
- Doesn't affect $R^2$ 
- Be sceptical of surprising results 


As we add more explanatory variables to our model, it becomes more likely that we will encounter multicollinearity. A full discussion of multicollinearity and how you can work around it is beyond the scope of this tutorial, but you should be familiar with the basic problem. 

The main problem with multicollinearity is that it makes the coefficient estimates unstable. This means that small changes to the values in one variable (say, changing the food rating of a few restaurants) can result in dramatic changes to the best-fit coefficients. This instability makes our results less robust. However, multicollinearity does not compromise the explanatory power of the model as a whole. The $R^2$ values are still valid. 

While there are a number of approaches to diagnosing and correcting for multicollinearity, at this stage you should exercise great caution when interpreting coefficients in a model where you have highly correlated explanatory variables. If you see results that seem strange, counterintuitive, or surprising, it might be that multicollinearity is to blame.

### Parallel planes with location

We have explored models that included the quality of both food and service, as well as location, but we haven't put these variables all into the same model. Let's now build a parallel planes model that incorporates all three variables. 

Examine the coefficients closely. Do they make sense based on what you understand about these data so far? How did the coefficients change from the previous models that you fit?


- Use `lm()` to fit a parallel planes model for `Price` as a function of `Food`, `Service`, and `East` from the dataset `nyc`.


```{r ex5, exercise=TRUE}
# Price by Food and Service and East

```

```{r ex5-hint}
lm(Price ~ ___ + ___ + ___, data = nyc)
```

```{r ex5-solution}
# Price by Food and Service and East
lm(Price ~ Food + Service + East, data = nyc)
```

### Interpretation of location coefficient


The fitted coefficients from the parallel planes model are listed below.

```{r}
lm(Price ~ Food + Service + East, data = nyc)
```




Reason about the magnitude of the `East` coefficient.

*Hint:* Does being on the East side increase or decrease the price of a meal?


```{r mc3}
question("Which of the following statements is **FALSE**?",
  answer("The premium for being on the East side of 5th Avenue is just less than a dollar, after controlling for the quality of food and service.", message = "This message is TRUE"),
  answer("The impact of location is relatively small, since one additional rating point of either food or service would result in a higher expected price than moving a restaurant from the West side to the East side. ", message = "This message is TRUE"),
  answer("The expected price of a meal on the East side is about 96% of the cost of a meal on the West side, after controlling for the quality of food and service.", correct = TRUE),
  allow_retry=TRUE
)
```




### Impact of location


The impact of location brings us to a modeling question: should we keep this variable in our model? In a later tutorial, you will learn how we can conduct formal hypothesis tests to help us answer that question. In this tutorial, we will focus on the size of the effect. Is the impact of location big or small? 

One way to think about this would be in terms of the practical significance. Is the value of the coefficient large enough to make a difference to your average person? The units are in dollars so in this case this question is not hard to grasp. 

Another way is to examine the impact of location in the context of the variability of the other variables. We can do this by building our parallel planes in 3D and seeing how far apart they are. Are the planes close together or far apart? Does the `East` variable clearly separate the data into two distinct groups? Or are the points all mixed up together?

- Use `plot_ly` to draw 3D scatterplot for `Price` as a function of `Food`, `Service`, and `East` by mapping the `z` variable to the response and the `x` and `y` variables to the numeric explanatory variables. Use color to indicate the value of `East`. Place `Food` on the x-axis and `Service` on the y-axis.
- Use `add_surface()` (twice) to draw two planes through the cloud of points, one for restaurants on the West side and another for restaurants on the East side. Use the objects `plane0` and `plane1`.

```{r ex6-setup}
mod <- lm(Price ~ Food + Service + East, data = nyc)

grid <- nyc |>
  modelr::data_grid(
    Food = seq_range(Food, n = 50),
    Service = seq_range(Service, n = 50),
    East
  )

tidy_planes <- mod |>
  augment(newdata = grid)

x <- unique(grid$Food)
y <- unique(grid$Service)

plane0 <- tidy_planes |>
  filter(East == 0) |>
  pull(.fitted) |>
  matrix(nrow = length(x), byrow = TRUE)
  
plane1 <- tidy_planes |>
  filter(East == 1) |>
  pull(.fitted) |>
  matrix(nrow = length(x), byrow = TRUE)
  
col1 <- c('#BF382A', '#0C4B8E')
col3 <- colorRamp(c("orange"))
```

```{r ex6, exercise=TRUE}
# draw 3D scatterplot
p <- plot_ly(data = nyc, z = ~___, x = ~___, y = ~___, opacity = 0.6) |>
  add_markers(color = ~factor(East)) 

# draw two planes
p |>
  add_surface(x = ~x, y = ~y, z = ~___, showscale = FALSE) |>
  add_surface(x = ~x, y = ~y, z = ~___, showscale = FALSE)
```

<div id="ex6-hint">
**Hint:** In `add_surface()` `z` should be `~plane0` in one layer and `~plane1` in the other.
</div>

```{r ex6-solution}
# draw 3D scatterplot
p <- plot_ly(data = nyc, z = ~Price, x = ~Food, y = ~Service, opacity = 0.6) |>
  add_markers(color = ~factor(East)) 

# draw two planes
p |>
  add_surface(x = ~x, y = ~y, z = ~plane0, showscale = FALSE) |>
  add_surface(x = ~x, y = ~y, z = ~plane1, showscale = FALSE)
```

### Full model

One variable we haven't considered is `Decor`. Do people, on average, pay more for a meal in a restaurant with nicer decor? If so, does it still matter after controlling for the quality of food, service, and location? 

By adding a third numeric explanatory variable to our model, we lose the ability to visualize the model in even three dimensions. Our model is now a hyperplane -- or rather, parallel hyperplanes -- and while we won't go any further with the geometry, know that we can continue to add as many variables to our model as we want. As humans, our spatial visualization ability taps out after three numeric variables (maybe you could argue for four, but certainly no further), but neither the mathematical equation for the regression model, nor the formula specification for the model in R, is bothered by the higher dimensionality. 

Use `lm()` to fit a parallel planes model for `Price` as a function of `Food`, `Service`, `Decor`, and `East` using the dataset `nyc`.

```{r ex7, exercise=TRUE}
# Price by Food and Service and East and Decor

```

```{r ex7-hint, exercise=TRUE}
lm(Price ~ ___ + ___ + ___ + ___, data = nyc)
```

```{r ex7-solution}
lm(Price ~ Food + Service + East + Decor, data = nyc)
```

Notice the dramatic change in the value of the `Service` coefficient. 

*Hint:* If the coefficient on `Service` changes so much, how likely is it that any interpretation relying on the estimate is valid?

```{r mc4}
question("Which of the following interpretations is invalid?
",
  answer("Since the quality of food, decor, and service were all strongly correlated, multicollinearity is the likely explanation.", message = "This is true"),
  answer("Once we control for the quality of food, decor, and location, the additional information conveyed by service is negligible. ", message = "This is true"),
  answer("Service is not an important factor in determining the price of a meal.", correct = TRUE, message = "Correct!"),
  answer("None of the above.", message = "Nope, try again."),
  allow_retry=TRUE
)
```


## Submit```{r encoder_ui, echo=FALSE}hash_encoder_ui = {    shiny::div("If you have completed this tutorial and are happy with all of your",         "solutions, please enter your identifying information, then click the button below to generate your hash",          textInput("name", "What's your name?"),        textInput("studentID", "What is your student ID?"),        renderText({ input$caption }),         )}``````{r encoder_logic, echo=FALSE, context="server"}is_server_context = function(.envir) {  # We are in the server context if there are the follow:  # * input - input reactive values  # * output - shiny output  # * session - shiny session  #  # Check context by examining the class of each of these.  # If any is missing then it will be a NULL which will fail.  inherits(.envir$input,   "reactivevalues") &  inherits(.envir$output,  "shinyoutput")    &  inherits(.envir$session, "ShinySession")}check_server_context = function(.envir) {  if (!is_server_context(.envir)) {    calling_func = deparse(sys.calls()[[sys.nframe()-1]])    err = paste0(      "Function `", calling_func,"`",      " must be called from an Rmd chunk where `context = \"server\"`"    )    stop(err, call. = FALSE)  }}encoder_logic = function(strip_output = FALSE) {  p = parent.frame()  check_server_context(p)  # Make this var available within the local context below  assign("strip_output", strip_output, envir = p)  # Evaluate in parent frame to get input, output, and session  local({    encoded_txt = shiny::eventReactive(      input$hash_generate,      {        # shiny::getDefaultReactiveDomain()$userData$tutorial_state        state = learnr:::get_tutorial_state()        shiny::validate(shiny::need(length(state) > 0, "No progress yet."))        shiny::validate(shiny::need(nchar(input$name) > 0, "No name entered."))        shiny::validate(shiny::need(nchar(input$studentID) > 0, "Please enter your student ID"))        user_state = purrr::map_dfr(state, identity, .id = "label")        user_state = dplyr::group_by(user_state, label, type, correct)        user_state = dplyr::summarize(          user_state,          answer = list(answer),          timestamp = dplyr::first(timestamp),          .groups = "drop"        )        user_state = dplyr::relocate(user_state, correct, .before = timestamp)        user_info = tibble(label = c("student_name", "student_id"), type="identifier", answer= as.list(c(input$name, input$studentID)))                  learnrhash::encode_obj(bind_rows(user_info, user_state))      }    )    output$hash_output = shiny::renderText(encoded_txt())  }, envir = p)}encoder_logic()``````{r encode, echo=FALSE}learnrhash::encoder_ui(ui_before = hash_encoder_ui)```

## Congratulations!

You have successfully completed Lesson 10 in Tutorial 3: Regression modeling.  

We hope that you've found this tutorial to be illuminating and useful. 

You learned about a variety of multiple regression models by focusing on the interplay between the mathematical, geometric, and syntactical representations of these models. You extended many of those same ideas to logistic regression, which covers the case in which we have a binary response variable. 

We discussed this material as a branch of descriptive statistics. That is, we focused on how the models work, how we should interpret them, and how they can be used, but we didn't talk at all about inference. Statisticians have developed many techniques for performing inference on the parameters of these regression models. Those techniques help us answer questions about whether the effects we observe are within the realm of statistical noise, or not. Without inference, we can only state how big the effect we observed was---we can't make any claims about whether that effect was likely the result of chance alone, or whether it represents a meaningful characterization of the underlying phenomenon. 

To complete your understanding, you should learn about inferential techniques for regression in the tutorial on Inference for Regression. 

What's next?

`r emo::ji("ledger")` [Full list of tutorials supporting OpenIntro::Introduction to Modern Statistics](https://openintrostat.github.io/ims-tutorials/)

`r emo::ji("spiral_notepad")` [Tutorial 3: Regression modeling](https://openintrostat.github.io/ims-tutorials/03-model/)

`r emo::ji("one")` [Tutorial 3 - Lesson 1: Visualizing two variables](https://openintro.shinyapps.io/ims-03-model-01/)

`r emo::ji("two")` [Tutorial 3 - Lesson 2: Correlation](https://openintro.shinyapps.io/ims-03-model-02/)

`r emo::ji("three")` [Tutorial 3 - Lesson 3: Simple linear regression](https://openintro.shinyapps.io/ims-03-model-03/)

`r emo::ji("four")` [Tutorial 3 - Lesson 4: Interpreting regression models](https://openintro.shinyapps.io/ims-03-model-04/)

`r emo::ji("five")` [Tutorial 3 - Lesson 5: Model fit](https://openintro.shinyapps.io/ims-03-model-05/)

`r emo::ji("six")` [Tutorial 3 - Lesson 6: Parallel slopes](https://openintro.shinyapps.io/ims-03-model-06/)

`r emo::ji("seven")` [Tutorial 3 - Lesson 7: Evaluating and extending parallel slopes model](https://openintro.shinyapps.io/ims-03-model-07/)

`r emo::ji("eight")` [Tutorial 3 - Lesson 8: Multiple regression](https://openintro.shinyapps.io/ims-03-model-08/)

`r emo::ji("nine")` [Tutorial 3 - Lesson 9: Logistic regression](https://openintro.shinyapps.io/ims-03-model-09/)

`r emo::ji("keycap_ten")` [Tutorial 3 - Lesson 10: Case study: Italian restaurants in NYC](https://openintro.shinyapps.io/ims-03-model-10/)

`r emo::ji("open_book")` [Learn more at Introduction to Modern Statistics](http://openintro-ims.netlify.app/)
